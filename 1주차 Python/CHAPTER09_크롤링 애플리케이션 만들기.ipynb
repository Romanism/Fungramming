{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER9 크롤링 애플리케이션 만들기\n",
    "\n",
    "- 스크래피\n",
    "- 설치\n",
    "- 스파이더 만들기\n",
    "- 스파이더 규칙 설정하기\n",
    "- 파서 함수 정의하기\n",
    "- 완성된 스파이더 클래스\n",
    "- 크롤링 GO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 스크래피 (Scrapy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": 크롤링 프레임워크를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 장점\n",
    "\n",
    "- 스크랩할 항목 유형을 정의하는 클래스를 만들 수 있습니다.\n",
    "- 수집한 데이터를 원하는 대로 편집하는 기능을 제공합니다.\n",
    "- 서버에 연동하기 위해 기능을 확장할 수 있습니다.\n",
    "- 크롤링 결과를 JSON, XML, CSV 등의 형식으로 내보낼 수 있습니다.\n",
    "- 손상된 HTML 파일을 분석할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 과정\n",
    "\n",
    "1. 크롤링할 아이템을 설정합니다.\n",
    "2. 실제 크롤링할 스파이더(Spider, 스크래피의 크롤러)를 만듭니다.\n",
    "3. 크롤링할 사이트(시작점)와 크롤링 규칙을 설정합니다.\n",
    "4. 스파이더의 종류에 따른 몇가지 설정을 추가합니다. ex) 크롤링할 URL의 패턴등을 설정\n",
    "5. HTML 문서를 파싱한 후 크롤러가 실행할 작업을 정의합니다.\n",
    "6. 크롤러를 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install scrapy\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "# 패키지 설치\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.1 스크래피 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'hanbit_media', using template directory '/anaconda3/lib/python3.6/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/ji/Desktop/Git/20_Fungramming/개발자를 위한 파이썬/hanbit_media\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd hanbit_media\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "# 프로젝트 폴더 생성\n",
    "! scrapy startproject hanbit_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hanbit_media\r\n",
      "├── hanbit_media\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── __pycache__\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── spiders\r\n",
      "│       ├── __init__.py\r\n",
      "│       └── __pycache__\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "4 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "# 프로젝트 폴더 확인\n",
    "! tree hanbit_media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.2 아이템 선정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": hanbit_media 폴더에 있는 items.py 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 스파이더 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hanbit_media 폴더로 진입합니다. \n",
    "- cmd창에 'scrapy genspider -t crawl <파이썬 이름> <크롤링 할 주소>'입력합니다.\n",
    "- 주의해야 할 점은 주소를 입력할 때 'http://'와 'www'를 생략해야 한다는 것입니다.\n",
    "- 실행하면 hanbit_media 폴더 내에 Spiders 디렉토리가 생긴것을 확인할 수 있습니다.\n",
    "- 파일 수정을 확인하기 위해서 hnabit_media 폴더 - spiders 폴더 - book_crawl.py 파일 참고하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 스파이더 규칙 설정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hnabit_media 폴더 - spiders 폴더 - book_crawl.py 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 파서 함수 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hnabit_media 폴더 - spiders 폴더 - book_crawl.py 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 완성된 스파이더 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hnabit_media 폴더 - spiders 폴더 - book_crawl.py 파일 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.7 크롤링 GO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    scrapy crawl book_crawl -o book_list.csv -t csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
